# -*- coding: utf-8 -*-
"""Final 02 - EDA Exercises.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QgcBZj3L5MEUtMHe1JelBO-11sfaOvmQ

#This notebook will permit you to manipulate Python libraries. It is divided into three parts. 

* Data Analysis and Manipulation (pandas)
* Data Preparation (Matplolib)
* Data Visualization (NumPy)

#Part I - Pandas
###Pandas is a fast, powerful, flexible and easy-to-use open source data analysis and manipulation library, built on top of the Python programming language.

###Description of the data

Titanic Data Set

The data set that we will be using is taken from Kaggle. The "***titanic-train.csv***" https://www.kaggle.com/hesh97/titanicdataset-traincsv file contains data for ***897*** of the real Titanic passengers. Each row represents one person. The columns describe different attributes about the passenger which includes:

1. Passesnger ID
2. Whether they Survived
3. Passenger Class
4. Name
5. Sex
6. Age
7. Siblings Aboard
8. Parents Aboard
9. Ticket
10. Fare paid in Â£s
11. Cabin
12. Port of Embarkation 

On ***April 15, 1912***, the largest passenger liner ever made collided with an iceberg during her maiden voyage. When the Titanic sank it killed ***1502*** out of ***2224*** passengers and crew. This sensational tragedy shocked the international community and led to *better safety regulations for ships*. One of the reasons that the shipwreck resulted in such loss of life was that there were not enough *lifeboats *for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others.
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing libraries
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
# %matplotlib inline

# Connecting Google Drive Data Files to Google Colab Notebook.
# Load the Google Drive helper and mount the drive.
from google.colab import drive

# This will prompt for authorization. Click the link and provide the required information.

drive.mount('/content/drive')

# List the content of YOUR google drive
!ls "/content/drive/My Drive"

# In this case, data is stored in the directory described below
# Change the directory based on YOUR own Google Drive organization
df = pd.read_csv('/content/drive/My Drive/Deep Learning/DEEP LEARNING MATERIAL/Data Sets/titanic-train.csv') #pd.read_csv() -> Read the data from specified location and stores in dataframe 'df'

type(df) # Type is used to check the type of a variable. In this case, df is a DataFrame.

df.head() # Displaying the header which by default is the first 5 lines of the data.

df.info() 
# Gives information about the Data Frame. This is a summary of the data containing the total number of rows and columns and their types. 
# We can also detect if there are missing data.
# object are string data
# Example in 'Age' the total number of rows data is 714 which means that there are missing records as the total number of entries are 891.

df.describe() # Statistical summary of numeric data - minmum, maximum, std, mean etc.

"""###Indexing
Pandas allows to index data in various ways.
"""

df.iloc[3] # To retrieve the records based on the index. The index starts with 0 in Python. Here we get the data in the 4th row.

df.loc[0:4,'Ticket'] # To retreive the data for one coloumn and several rows.
                     # Here we get the Ticket colum of the first 5 rows.

df['Ticket'].head() # You can get first five rows of the colum Ticket with the help of head() function as well.

type(df[['Embarked', 'Ticket']]) # It is useful to check the types of expressions before using them.

df[['Embarked', 'Ticket']].head() # You can get multiple coloumn data by providing the list as parameter. 
                                  #Here you get data of the 'Embarked' and 'Tickets' columns

"""###Selections
Pandas allows to perform different selections.
"""

df[df['Age'] > 65] # We can get the data of passengers whose age is above 65. To get the details information 
                   # We pass the condition to the DataFrame in square brackets. 
                   # Otherwise, you will get a list of values which says if the condition is true or false.

df['Age'] > 65 # This statement displays if the condition is True or False.

df.query('Age > 65') # Query can also be used to select the data based on the condition passed as a string

df[(df['Age'] == 10) & (df['SibSp'] == 5)]  # Conditions can be combined with Boolean operators like AND, OR, and NOT. 
                                            # In this example we have combined two conditions.
                                            # Condition : Selection of passengers with 'Age' 10 AND 5 'siblings'. 
                                            # The condition is False so nothing is displayed.

df[(df.Age == 10) | (df.SibSp == 5)]       # Here we are using OR.

df.query('(Age == 10) | (SibSp == 5)') # Similarly, we can use query.

"""###Unique Values"""

df['Embarked'].unique() # We can get the unique values from a column.

"""###Sorting"""

df.sort_values('Age', ascending = False).head() # Sort the data in DataFrame either in ascending or descending order for a column.

"""###Aggregations
Like in SQL, Pandas allows you to do aggregations and group by.
"""

df['Survived'].value_counts() # You can aggregate the data to get the number of survivors. 
                              # '0's' and '1's' values in the Survived field determine if a person survived or not.

df['Pclass'].value_counts() # Number of people per class

df.groupby(['Pclass', 'Survived'])['PassengerId'].count() # Number of survivors per class

df['Age'].min()

df['Age'].max()

df['Age'].mean()

df['Age'].median()

# Information about the survivors

mean_age_by_survived = df.groupby('Survived')['Age'].mean()
mean_age_by_survived

std_age_by_survived = df.groupby('Survived')['Age'].std()
std_age_by_survived

min_age_by_survived = df.groupby('Survived')['Age'].min()
min_age_by_survived

max_age_by_survived = df.groupby('Survived')['Age'].max()
max_age_by_survived

"""###Merging Two Data Frames."""

df1 = mean_age_by_survived.round(0).reset_index()
df2 = std_age_by_survived.round(0).reset_index()

df1

df2

df3 = pd.merge(df1, df2, on='Survived')

df3

df3.columns = ['Survived', 'Age Mean', 'Age Standard Deviation']

df3

"""###Pivot Tables
You can reshuffle the data in Pivot tables similar to Excel PivotTables.
"""

df.pivot_table(index='Pclass',
               columns='Survived',
               values='PassengerId',
               aggfunc='count')

"""###Correlations
In Pandas you can also calculate correlations between features.
"""

correlated_with_survived = df.corr()['Survived'].sort_values()
correlated_with_survived

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

correlated_with_survived.iloc[:-1].plot(kind='bar',
                                        title='Titanic Passengers: correlation with survival')

"""#Part II - Visual Data Exploratory Analysis with Matplotlib

Matplotlib is a Python 2D plotting library which produces quality figures in a variety of hardcopy formats.
"""

# Generating some random data for visualization!
data1 = np.random.normal(0, 0.1, 1000)
data2 = np.random.normal(1, 0.4, 1000) + np.linspace(0, 1, 1000)
data3 = 2 + np.random.random(1000) * np.linspace(1, 5, 1000)
data4 = np.random.normal(3, 0.2, 1000) + 0.3 * np.sin(np.linspace(0, 20, 1000))

data = np.vstack([data1, data2, data3, data4]).transpose() #with transpose() - Transposing the data.

df = pd.DataFrame(data, columns=['data1', 'data2', 'data3', 'data4'])
df.head()

df.info()

"""###Line Plot"""

df.plot(title='Line plot')

plt.plot(df)
plt.title('Line plot')
plt.legend(['data1', 'data2', 'data3', 'data4'])

"""###Scatter Plot"""

df.plot(style='.')

_ = df.plot(kind='scatter', x='data1', y='data2',
            xlim=(-1.5, 1.5), ylim=(0, 3))

"""###Histograms"""

df.plot(kind='hist',
        bins=50,
        title='Histogram',
        alpha=0.6)

"""###Cumulative distribution"""

df.plot(kind='hist',
        bins=100,
        title='Cumulative distributions',
        #normed=True,
        cumulative=True,
        alpha=0.4)

"""###Box Plot"""

df.plot(kind='box',
        title='Boxplot')

"""###Subplots"""

fig, ax = plt.subplots(2, 2, figsize=(5, 5))

df.plot(ax=ax[0][0],
        title='Line plot')

df.plot(ax=ax[0][1],
        style='o',
        title='Scatter plot')

df.plot(ax=ax[1][0],
        kind='hist',
        bins=50,
        title='Histogram')

df.plot(ax=ax[1][1],
        kind='box',
        title='Boxplot')

plt.tight_layout()

"""###Pie charts"""

gt01 = df['data1'] > 0.1
piecounts = gt01.value_counts()
piecounts

piecounts.plot(kind='pie',
               figsize=(5, 5),
               explode=[0, 0.15],
               labels=['<= 0.1', '> 0.1'],
               autopct='%1.1f%%',
               shadow=True,
               startangle=90,
               fontsize=16)

"""###Hexbin plot"""

data = np.vstack([np.random.normal((0, 0), 2, size=(1000, 2)),
                  np.random.normal((9, 9), 3, size=(2000, 2))])
df = pd.DataFrame(data, columns=['x', 'y'])

df.head()

df.plot()

df.plot(kind='kde')

df.plot(kind='hexbin', x='x', y='y', bins=100, cmap='rainbow')

"""#Part III - Numpy

###Linear Algebra with Numpy
"""

a = np.array([1, 3, 2, 4])

a

type(a)

A = np.array([[3, 1, 2],
              [2, 3, 4]])

B = np.array([[0, 1],
              [2, 3],
              [4, 5]])

C = np.array([[0, 1],
              [2, 3],
              [4, 5],
              [0, 1],
              [2, 3],
              [4, 5]])

print("A is a {} matrix".format(A.shape))
print("B is a {} matrix".format(B.shape))
print("C is a {} matrix".format(C.shape))
print("A is a " + str(A.shape) + " matrix")

A[0]

C[2, 0]

B[:, 0]

"""###Elementwise operations"""

3 * A

A + A

A * A

A / A

A - A

"""###Dot product"""

A.shape

B.shape

A.dot(B) # Product of matrices

np.dot(A, B)

B.dot(A)

C.shape

A.shape

C.dot(A)

"""###Unstructured data: images, sounds etc.

Here we will explore images and sounds.

###Images
"""

from PIL import Image

# Change the path to YOUR image
img = Image.open('/content/drive/My Drive/Deep Learning/DEEP LEARNING MATERIAL/Data Sets/WTC.jpg')
img

type(img)

imgarray = np.asarray(img)

type(imgarray)

imgarray.shape # 3 represents the RGB

imgarray.ravel().shape

2330 * 2250 * 3

"""###Sound"""

from scipy.io import wavfile

rate, snd = wavfile.read(filename='/content/drive/My Drive/Deep Learning/DEEP LEARNING MATERIAL/Data Sets/sms.wav')

from IPython.display import Audio

Audio(data=snd, rate=rate)

len(snd)

snd

plt.plot(snd)

_ = plt.specgram(snd, NFFT=1024, Fs=44100)
plt.ylabel('Frequency (Hz)')
plt.xlabel('Time (s)')