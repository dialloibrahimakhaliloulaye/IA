{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbotProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dialloibrahimakhaliloulaye/IA/blob/master/chatbotProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAr2joSq3ky2",
        "colab_type": "text"
      },
      "source": [
        "# Programmation d'un simple Chatbot de Scratch en Python (utilisation de NLTK)\n",
        "\n",
        "\n",
        "![Alt text](https://cdn-images-1.medium.com/max/800/1*pPcVfZ7i-gLMabUol3zezA.gif)\n",
        "\n",
        "l'histoire des chatbots remonte à 1966 quand un programme informatique appelé ELIZA a été inventé par Weizenbaum. Il a imité le langage d'un psychothérapeute à partir de seulement 200 lignes de code. Vous pouvez toujours en parler ici: [Eliza](http://psych.fullerton.edu/mbirnbaum/psych101/Eliza.htm?utm_source=ubisend.com&utm_medium=blog-link&utm_campaign=ubisend). \n",
        "\n",
        "Sur des lignes similaires, créons un chatbot très basique utilisant la bibliothèque NLTK de Python.C'est un bot très simple avec presque aucune compétence cognitive, mais toujours un bon moyen d'entrer en NLP et de se familiariser avec les chatbots.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vF4LvLVSMLG",
        "colab_type": "text"
      },
      "source": [
        "## NLP\n",
        "La NLP est un moyen pour les ordinateurs d'analyser, de comprendre et de tirer un sens du langage humain d'une manière intelligente et utile. En utilisant la PNL, les développeurs peuvent organiser et structurer leurs connaissances pour effectuer des tâches telles que la synthèse automatique, la traduction, la reconnaissance d'entités nommées, l'extraction de relations, l'analyse de sentiments, la reconnaissance vocale et la segmentation de sujets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_3TYqNuTRfx",
        "colab_type": "text"
      },
      "source": [
        "##Importations des librairies nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF04GfD9Sma1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import string # to process standard python strings\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVqjsxEWUdZ_",
        "colab_type": "text"
      },
      "source": [
        "## Lire dans le corpus\n",
        "\n",
        "Pour notre exemple, nous utiliserons la page Wikipedia pour les chatbots comme corpus. Copiez le contenu de la page et placez-le dans un fichier texte nommé ‘racisme.txt’ que nous allons placer dans le drive (content/dirive/My drive/racisme.txt). Cependant, vous pouvez utiliser n'importe quel corpus de votre choix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4RWJPrzUnEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "1ecec881-f2aa-4bb2-b124-761de7e594d9"
      },
      "source": [
        "f=open('/content/drive/My Drive/racisme.txt','r',errors = 'ignore')\n",
        "raw=f.read()\n",
        "raw=raw.lower()# converts to lowercase\n",
        "nltk.download('punkt') # first-time use only\n",
        "nltk.download('wordnet') # first-time use only\n",
        "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
        "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQWksFBhT6qU",
        "colab_type": "text"
      },
      "source": [
        "## Droit d'accès au drive\n",
        "on doit d'abord monter le drive ensuite donner les autorisations d'accès à notre fichier 'racisme.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujRZMinF59ye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd606246-fab5-451a-fce7-7653b8eb84f6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDTidQD2ZQdo",
        "colab_type": "text"
      },
      "source": [
        "## La tokenisation\n",
        "on doit envoyé les tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVz_QZ7JZa0v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2059ba63-3a2c-447c-c599-68e5e2278141"
      },
      "source": [
        "sent_tokens[:2]\n",
        "['a chatbot (also known as a talkbot, chatterbot, bot, im bot, interactive agent, or artificial conversational entity) is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.',\n",
        " 'such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the turing test.']\n",
        "word_tokens[:2]\n",
        "['a', 'chatbot', '(', 'also', 'known']"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'chatbot', '(', 'also', 'known']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIWQ0zWaZjDt",
        "colab_type": "text"
      },
      "source": [
        "##Prétraitement\n",
        "Nous allons maintenant définir une fonction appelée LemTokens qui prendra en entrée les jetons et retournera les jetons normalisés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2WclGy7ZsTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eusJx5fJZynz",
        "colab_type": "text"
      },
      "source": [
        "##Matching des mots clés\n",
        "Ensuite, nous allons définir une fonction pour un message d'accueil par le bot, c'est-à-dire que si la saisie d'un utilisateur est un message d'accueil, le bot doit retourner une réponse de salutation.ELIZA utilise un simple mot clé correspondant pour les salutations. Nous utiliserons ici le même concept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_EsJnPqZ5Zv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GREETING_INPUTS = (\"Bonjour\", \"merci\", \"salut\", \"slt\", \"vous allez bien\",\"ca va\",\"Je vous remercie\")\n",
        "GREETING_RESPONSES = [\"assalamou aleykoum\", \"ca va\", \"*nods*\", \"bonjour ici\", \"hello\", \"bienvenue chez moi, merci de poser vos questions\"]\n",
        "def greeting(sentence):\n",
        " \n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkYmJvs1apM6",
        "colab_type": "text"
      },
      "source": [
        "## Génération d'une réponse\n",
        "\n",
        "### Bloc de mots\n",
        "Après la phase initiale de prétraitement, nous devons transformer le texte en un vecteur (ou tableau) significatif de nombres. Le sac de mots est une représentation de texte qui décrit l'occurrence de mots dans un document. Cela implique deux choses:\n",
        "\n",
        "* Un vocabulaire de mots connus.\n",
        "\n",
        "* Une mesure de la présence de mots connus.\n",
        "\n",
        "Pourquoi est-il appelé un «sac» de mots? En effet, toute information sur l'ordre ou la structure des mots dans le document est supprimée et le modèle est uniquement ** concerné par la question de savoir si les mots connus se trouvent dans le document, pas où ils se trouvent dans le document. **\n",
        "\n",
        "L'intuition derrière le sac de mots est que les documents sont similaires s'ils ont un contenu similaire. En outre, nous pouvons apprendre quelque chose sur la signification du document à partir de son seul contenu.\n",
        "\n",
        "Par exemple, si notre dictionnaire contient les mots {Learning, is, the, not, great}, et que nous voulons vectoriser le texte \"Learning is great\", nous aurions le vecteur suivant: (1, 1, 0, 0, 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mkoBef1bJIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46UiUuvFbnsM",
        "colab_type": "text"
      },
      "source": [
        "import cosine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu8cQuD7bOr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp24F9f4bt0j",
        "colab_type": "text"
      },
      "source": [
        "Pour générer une réponse de notre bot aux questions d'entrée, le concept de similitude des documents sera utilisé. Nous définissons une réponse de fonction qui recherche dans l'énoncé de l'utilisateur un ou plusieurs mots clés connus et renvoie l'une des réponses possibles. S'il ne trouve pas l'entrée correspondant à l'un des mots clés, il renvoie une réponse: \"Je suis désolé! Je ne te comprends pas \""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owkk3ztbbs2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"Désolé! mais je ne vous comprends pas, merci de réessayer\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yp5ZXGEcUrx",
        "colab_type": "text"
      },
      "source": [
        "##Lignes d'alimentation\n",
        "Enfin, nous alimenterons les lignes que nous voulons que notre bot dise tout en commençant et en terminant une conversation en fonction de l'entrée de l'utilisateur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nisRUhecSlg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "f2dc5398-856f-428d-8fb9-78190f3974cf"
      },
      "source": [
        "flag=True\n",
        "print(\"ROBO: Mon nom est Robo. Je vais essayer de répondre à vos questions dans le Chatbots. Si vous voulez quitez, tapez Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='merci' or user_response=='Je vous remercie' ):\n",
        "            flag=False\n",
        "            print(\"ROBO: Grand merci à vous..\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"ROBO: \"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"ROBO: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"ROBO: Bye! A plus..\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROBO: Mon nom est Robo. Je vais essayer de répondre à vos questions dans le Chatbots. Si vous voulez quitez, tapez Bye!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ROBO: etymologie\n",
            "selon le cnrtl, le mot racisme serait apparu en 19028 alors que le mot raciste daterait de 18929.\n",
            "\n",
            "\n",
            "selon charles maurras  ; gaston méry (1866-1909), pamphlétaire, journaliste collaborateur a la libre parole — le journal \n",
            "antisémite et polémiste d'edouard drumont — est la première personne connue à avoir utilisé le mot « raciste » en\n",
            " 189411,12,13.\n",
            "\n",
            "\n",
            "toutefois l'adjectif « raciste »14 et le nom « racisme » ne s'installent dans le vocabulaire général en france qu'à \n",
            "partir des années 193015. léon trotsky l'emploie en 1930 dans son histoire de la révolution russe, avec un sens culturel \n",
            "pour qualifier le groupe des slaves traditionalistes qui défendent leur culture et leur mode de vie national.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}